{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# главная ф-я - проходит по каждой странице (на каждой странице ищет 18 ссылок на статьи, парсит дату, название статьи, кто опубликовал)\n",
    "def main_parser():\n",
    "    last_date = \"2017-06-08\" # последняя дата - критерий остановки парсинга\n",
    "    page_num = 0 # начальный номер страницы\n",
    "    flag = True # флаг равен True, пока дата не последняя\n",
    "    with open('articles_2017_2018_June.csv', 'w+', encoding='utf-8', newline='') as csv_file: # создаем/открываем .csv файл для записи\n",
    "        while flag: # пока дата не последняя\n",
    "            page_num+=1 # увеличиваем номер страницы\n",
    "            url = \"https://www.kyivpost.com/page/\"+str(page_num)+\"/?s&time=all&section=all&type=all&exclusive=no&authors=all\" # втсавляем номер страницы в ссылку\n",
    "            req = request.Request(url, headers={'User-Agent' : \"Magic Browser\"})  # параметр для http запроса, который говорит что ты не робот\n",
    "            html = request.urlopen(req).read().decode('utf8') # скачиваем html код страницы с номером page_num\n",
    "            soup = BeautifulSoup(html, \"lxml\") # превращаем полученную строку с html-кодом в обьект BeatifulSoup, чтобы легко было парсить\n",
    "            article_oject = soup.findAll('div', {'class':\"grid-3 post-excerpt post-excerpt-simple\"}) # находим те участки html-кода, которые отображают статьи - получаем список таких участков - то есть список всех статей на этой странице по сути\n",
    "            for obj in article_oject: # проходимся по каждому такому участку и для каждой статьи\n",
    "                datetime = obj.find('time')['datetime'] # парсим дату\n",
    "                if datetime.startswith(last_date): # проверяем, не последняя ли дата\n",
    "                    break # если последняя - парсинг остановится\n",
    "                article_title = obj.find('h2', {'class': 'title'}).text # парсим название статьи\n",
    "                article_link = obj.find('a')['href'] # парсим ссылку на статью\n",
    "                published_by = obj.find('div', {'class':\"author mt-bigger\"}).text # парсим, кто опубликовал\n",
    "                if article_link.endswith('html'): # дополнительная проверка во избежание ошибок - потом обьясню\n",
    "                    article_text = get_article(article_link) # используем еще одну написанную нами ф-ю, чтобы достать текст статьи\n",
    "                    if article_text != 0: # если функция нашла текст статьи то записываем строку в .csv файл\n",
    "                        row = [article_title.replace('\\n', ''), published_by, datetime[:10],  article_text,  article_link.replace('\\n\\n', '\\n')]\n",
    "                        wr = csv.writer(csv_file, dialect='excel', delimiter = \";\")\n",
    "                        wr.writerow(row)\n",
    "                        print (article_link)\n",
    "            print (\"----------\", page_num, \"--------------\")\n",
    "            print (\"----------\", datetime[:10], \"--------------\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вспомагательная ф-я, чтобы спарсить текст статьи по переданной ссылке\n",
    "def get_article(url):\n",
    "    req = request.Request(url, headers={'User-Agent' : \"Magic Browser\"}) \n",
    "    html = request.urlopen(req).read().decode('utf8') # скачиваем html-код страницы со статьей \n",
    "    soup = BeautifulSoup(html, \"lxml\") # превращаем полученную строку с html-кодом в обьект BeatifulSoup, чтобы легко было парсить\n",
    "    article_obg = soup.find('div', {'class': \"content-block\"}) # находим ту часть кода, где находится статья\n",
    "    if article_obg is None: # если такой части не нашли, возвращаем 0\n",
    "        return 0\n",
    "    article_parts = article_obg.findAll('p') # находим все адзацы статьи \n",
    "    final_article = \"\"\n",
    "    for tag in article_parts: # соединяем все абзацы в одну строку и возвращаем как текст статьи\n",
    "        final_article+=' '+tag.text\n",
    "    return final_article.replace(\"Found a spelling error? Let us know – highlight it and press Ctrl + Enter.\", '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаеш парсер тут\n",
    "main_parser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
